AI & Genomics, the history of nlp architectures, and how transformers and LLMs apply to biological data

The human body runs on its own biological computer with DNA as its code. At compile time, DNA synthesizes from its original form to mRNA, converting genetic information into proteins. mRNA is code for proteins synthesized at the ribosome. Since DNA and mRNA occur in sequences, as has been helpful for the human genome project and other projects looking to study human biology from a genomic perspective, AI can also play a role in learning about our coding and that of other organisms.

Enter Natural Language Processing. Since proteins, DNA sequences, and their encodings are all mathematical objects (or can be treated as such) from AI's perspective, we can directly apply tools from Natural Language Processing (a field broadly referring to the application of artificial intelligence to language data) in bioinformatics contexts. In other words, a signal processing problem, like deciphering voices in a noisy room can be seen as analogous to a bioinformatics problem like unraveling the multiple sequences in a meta genome. Moreover, an AI that can understand what a person might say next (made possible via a transformer architecture--covered below--trained on a corpus of text) can also predict how a virus will evolve next if trained on a bioinformatics dataset.

Several groups have been jumping at this conclusion and the opportunity to use AI language systems, namely Large Language Models (LLMs) for the study of human biology. Recent advances in AI and LLMs are using massive text datasets to better understand human language and gain insight into context and sequences of text. But these advances aren't limited to human language. 

Recent applications of LLMs let AI learn the languages of biology and chemistry as well. By making it easier to train massive neural networks on biomolecular data, NVIDIA BioNeMO (link: https://blogs.nvidia.com/blog/2022/09/20/bionemo-large-language-models-drug-discovery/), for instance, helps researchers discover new patterns and insights in biological sequences -- insights that can connect to biological properties, functions, and human health conditions.

Let's take a look at how this is possible.

First, what is a LLM? LLMs are machine learning algorithms that use enormous text datasets (hence the models are LARGE-- often trained on hundreds of gigabytes of text data, take Google's flant5 11b, which is trained with 11 BILLION parameters) to recognize, predict, and generate human language on the basis of predicting the next word in a sequence. OpenAI's GPT-3 (trained on 175 billion parameters) is a popular example.

We cannot understand LLMs without understanding how the technology came about. Before we go further into how LLMs are used, let's go on a tangent and cover the history of their predecessor architectures used for natural language processing to understand how and why LLMs came about. 


NLP History and the birth of the transformer (could be its own piece)


The basic feed-forward neural network isn't designed to keep track of sequential data. It maps each individual input to an output, which is helpful for tasks like classifying images, but fails on text due to its sequential nature. To process text, we must take into account sequences and the relationships between words and sentences. 

Before transformers were introduced in 2017 (Attention is All You Need link: https://arxiv.org/abs/1706.03762), Recurrent Neural Networks (RNN) and Long Short-Term Memory networks (LSTM) were popularly used for natural language processing. An RNN processes words by taking the first word and feeding back the result into the layer that processes the next word to keep track of the entire sentence-- hence recurrent. They worked well but were slow and could not take advantage of parallel computing hardware or graphics processing units (GPUs). They also exhibited something called "the vanishing gradient problem," where, as they got deeper into a text excerpt, the early words of the sequence gradually faded, which was problematic for long sequences of text (or DNA in our case). Finally, they could only capture the relations between a given word and those that preceded it (whereas in reality, the words following it also affect meaning). After RNNs came Long Short-Term Memory Networks (LSTMs). They solved the vanishing gradient problem and were able to handle longer text sequences, but were even slower than RNNs at training and still could not support parallel compute. 

Enter transformers in 2017. Transformers can learn context in sequential data. What does this mean? Well, transformer networks make two key contributions: they make it possible to process entire sequences in parallel (which speeds up deep learning), and most importantly, they introduce attention mechanisms, which make it possible to track relationships between words across long text sequences in both directions. This information is called context. [There is ample information online for those looking to learn more about transformers.] 

LLMs are massive transformers and have been used to generate poems, music, build websites, write papers, and even code. They can be used to generate text in all forms and are often characterized by their emergent properties, which are still being studied and explored.

Back to biology

So with this immense power to study and manipulate text with LLM neural networks, we turn to a different kind of sequence, DNA. LLMs are already making massive advances in protein and chemical structure study. Applications in drug discovery are underway.

In 2022, ProtGPT2 (https://www.nature.com/articles/s41467-022-32007-7) was announced as a deep unsupervised LLM for protein generation and design. This means a language model is being used for building novel proteins customized for specific purposes. These proteins hold potential to tackle environmental and biomedical problems. You can play with ProtGPT2+Alphafold for protein synthesis and structure prediction here https://huggingface.co/spaces/Gradio-Blocks/protGPT2_gradioFold. 


So what's next?

NVIDIA's BioNeMO framework promises to help scientists better understand disease and find therapies for patients by supporting chemistry, protein, DNA, and RNA formats, essentially turning LLMs into biomolecular LLMs. 

As the scale of LLMs grows (some are now being trained on upwards of 500 billion parameters), so too can the capabilities expand. The sheer size of the networks alleviates the need to train on custom, smaller, datasets for specific tasks and allows scientists to scale up their models and capture information about molecular structure and more. Several scientific institutes and private companies are already using the technology or signed up to do so.

I for one am excited to see where this field will go. 



